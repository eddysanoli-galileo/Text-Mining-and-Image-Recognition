{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Video 2 Recipe\r\n",
    "\r\n",
    "Programa capaz de extraer recetas de cocina a partir de la transcripción del audio de un video de cocina en Youtube. Útil para aquellas recetas que no cuentan con una transcripción en la descripción."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "# pip install --upgrade youtube-dl\r\n",
    "import youtube_dl as ydl\r\n",
    "\r\n",
    "# pip install webvtt-py\r\n",
    "import webvtt\r\n",
    "\r\n",
    "# Para capturar stdout como un string\r\n",
    "from io import StringIO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Video a Procesar"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# URL del video del que se desea extraer una receta\r\n",
    "video_url = \"https://www.youtube.com/watch?v=Vr-o01qiRYI\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chequeo Subtítulos\r\n",
    "\r\n",
    "Se solicitan los subtítulos de un video. Si el video no tiene subtítulos oficiales, la bandera \"has_subs\" se retorna como False."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Reemplaza el stdout normal por uno custom llamado \"mystdout\"\r\n",
    "old_stdout = sys.stdout\r\n",
    "sys.stdout = temp_stdout = StringIO()\r\n",
    "\r\n",
    "# Opciones de descarga:\r\n",
    "# - Listar los subtítulos disponibles\r\n",
    "# - No descargar el video\r\n",
    "download_options = {\r\n",
    "    'listsubtitles': True,\r\n",
    "    \"skip_download\": True\r\n",
    "}\r\n",
    "\r\n",
    "# Se realiza el request. El progreso del programa es guardado\r\n",
    "# en la variable \"mystdout\"\r\n",
    "with ydl.YoutubeDL(download_options) as video:\r\n",
    "    video.download([video_url])\r\n",
    "\r\n",
    "# Se vuelve a poner \"stdout\" como logger\r\n",
    "sys.stdout = old_stdout\r\n",
    "\r\n",
    "# Se guardan los logs generados en una variable\r\n",
    "ydl_logs = temp_stdout.getvalue()\r\n",
    "\r\n",
    "# Si los logs contienen el string \"has no subtitles\" se\r\n",
    "# setea una variable como \"False\" para indicar esto.\r\n",
    "if \"has no subtitles\" in ydl_logs:\r\n",
    "    has_subtitles = False\r\n",
    "    print(\"El video no tiene subtítulos.\")\r\n",
    "else:\r\n",
    "    has_subtitles = True\r\n",
    "    print(\"El video tiene subtítulos\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El video no tiene subtítulos.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creación Corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Opciones de descarga:\r\n",
    "# - Descargar todos los archivos como \"source\"\r\n",
    "# - Descargar los subtítulos ya dados por el video\r\n",
    "# - Obviar la descarga del video como tal.\r\n",
    "# - Se descargan los subtítulos del video (oficiales si existen, automáticos si no los hay)\r\n",
    "download_options = {\r\n",
    "    'outtmpl': './media/subs',\r\n",
    "    'writesubtitles': True,\r\n",
    "    'writeautomaticsub': True,\r\n",
    "    \"skip_download\": True\r\n",
    "}\r\n",
    "\r\n",
    "# Se realiza la descarga\r\n",
    "with ydl.YoutubeDL(download_options) as video:\r\n",
    "    video.download([video_url])\r\n",
    "\r\n",
    "# Se listan todos los archivos de la carpeta \"media\"\r\n",
    "files = os.listdir(\"./media\")\r\n",
    "\r\n",
    "# Se revisa si algún archivo contiene \"subs\" en su nombre\r\n",
    "# de ser así, se extrae su path.\r\n",
    "for file in files:\r\n",
    "    if \"subs\" in file:\r\n",
    "        subtitles_path = file\r\n",
    "        break\r\n",
    "\r\n",
    "# ======================\r\n",
    "# SUBTÍTULOS MANUALES\r\n",
    "# ======================\r\n",
    "\r\n",
    "if has_subtitles:\r\n",
    "    \r\n",
    "    # Corpus del video\r\n",
    "    video_corpus = \"\"\r\n",
    "\r\n",
    "    # Se extrae el texto del archivo .vtt  \r\n",
    "    for caption in webvtt.read(\"./media/\" + subtitles_path):\r\n",
    "\r\n",
    "        # Se eliminan:\r\n",
    "        # - Hard spaces (&nbsp;)\r\n",
    "        # - Newlines (\\n)\r\n",
    "        # - Leading and trailing spaces\r\n",
    "        caption = caption.text.replace(\"&nbsp;\", \" \")\r\n",
    "        caption = caption.replace(\"\\n\", \"\")\r\n",
    "        caption = caption.strip()\r\n",
    "\r\n",
    "        # Se agrega el string limpio al corpus\r\n",
    "        video_corpus = video_corpus + \" \" + caption\r\n",
    "\r\n",
    "# ======================\r\n",
    "# SUBTÍTULOS AUTOMÁTICOS\r\n",
    "# ======================\r\n",
    "\r\n",
    "else:\r\n",
    "    \r\n",
    "    # Se inicializa la string de captions y la lista de snippets\r\n",
    "    # a incluir en el corpus final\r\n",
    "    previous_caption = \"\"\r\n",
    "    caption_snippets = []\r\n",
    "\r\n",
    "    for i, caption in enumerate(webvtt.read(\"./media/\" + subtitles_path)):\r\n",
    "\r\n",
    "        caption = caption.text.replace(\"&nbsp;\", \" \")\r\n",
    "        caption = caption.replace(\"\\n\", \"\")\r\n",
    "        caption = caption.strip()\r\n",
    "\r\n",
    "        # Se agrega siempre la primera caption a los snippets\r\n",
    "        if i == 0:\r\n",
    "            caption_snippets.append(caption)\r\n",
    "\r\n",
    "        # Si la caption anterior es parte de la nueva caption\r\n",
    "        # se agrega la caption anterior a los snippets\r\n",
    "        elif previous_caption in caption:\r\n",
    "            caption_snippets.append(previous_caption)\r\n",
    "\r\n",
    "        # Se actualiza la caption anterior\r\n",
    "        previous_caption = caption\r\n",
    "\r\n",
    "    # Se eliminan los strings repetidos \r\n",
    "    # Se crea un diccionario que utiliza como llaves los strings del corpus\r\n",
    "    # dado que un diccionario no puede tener llaves repetidas, elimina las\r\n",
    "    # repetidas y las retorna ordenadas. Se puede hacer la misma operación \r\n",
    "    # usando \"sets\" pero retorna los elementos del corpus desordenados.\r\n",
    "    caption_snippets = list(dict.fromkeys(caption_snippets))\r\n",
    "\r\n",
    "    # Se unen todas las strings\r\n",
    "    video_corpus = \" \".join(caption_snippets)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[youtube] Vr-o01qiRYI: Downloading webpage\n",
      "[info] Writing video subtitles to: media\\subs.en.vtt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizando Corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(video_corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Music]  America I know what you're thinking do we really need a recipe for baked potatoes well here in the Test Kitchen we baked over 200 pounds of spuds to discover that very answer and today I'm here with the expert Ellie who's gonna show us why we do need a recipe Bridgette some crazy things are happening in the world with baked potatoes and it has to stop Oh No immediately first we're cooking our potatoes in the microwave not good I've done it not gonna I've done it's right and it cooks unevenly it cooks from the inside out we also cook our potatoes in foil I've done that too and it traps in all of the moisture and it doesn't give us a tasty potato and finally when we do get it in the oven to bake it we let it hang out on the counter forever and there's no fluffiness coming out of that potato at all but their greatest doorstops absolutely today we're gonna do my favorite thing one of the things we love to do in the Test Kitchen we're gonna brine potatoes we're gonna brine potatoes yes you said Brian potatoes okay now I remember when we started brining beans a few years ago and I was a little skeptical about that really works though changes their structure and how they hold on to moisture much like meat but no we're brining potatoes yes because not only do we want a delicious potato we want the skin to be crispy and flavorful I like to eat my whole potato I don't know about you that's true that's true yeah okay so let's start brining the first thing we're gonna do is poke about six holes in these potatoes but just do two on every side and this is how we get it ready for the brine flavor is gonna seep in seems gonna seep out that's a lot of things are gonna happen and russets our perfect choice for baked potatoes because they're one of the highest starch content of all the potatoes and higher the starch means fluffier potatoes all right now that we have our potatoes ready we're going to create our brine so we have half a cup of water here and we have two tablespoons of salt we're also brining potatoes because it's very hard to season a potato just normally the salt will roll right off if we put it in water it sticks to the potato so I'm gonna transfer these potatoes to a wire rack the wire rack will allow the potatoes are cooked evenly from bottom on all sides and the top and also it's just easier to get it from the oven we tested brining these potatoes for up to an hour and we found that just a few seconds in the Brian works so it's more about the skin than going inside the potato yes gotcha all right so now that we have our potatoes barn we're gonna put them in the oven at 450 degrees it's very important that we let them cook until they register 205 degrees internal temperature and that should take from 45 minutes to an hour I love this this is so Test Kitchen we're actually looking for an internal temperature of potatoes now first we brine them now we temp them Bridget it's been 45 minutes and our potatoes are ready to cool they actually look a little frosty it's like the salt dried on them you can actually see the salt crust yeah that's gonna be delicious later so we're going to temp our potatoes remember we're looking for an internal temperature of 205 temping the potato does two things first it lets us know that the potato is obviously done second pushing the thermometer in gives you a feel of the resistance if it slides right in you know it's ready to go so we're at temp and we're going to brush these potatoes with the tablespoon of vegetable oil this is what's going to give us that nice crispy skin in the end we also found out that if we brushed the potatoes with oil prematurely we got leathery skin because if it was put on too soon the oil would have trapped that moisture in absolutely so it would have been kind of chewy on the inside and leathery on that out just like another potato mistake one thing you don't want to do you know I'm usually a butter advocate you don't want to brush the baked potatoes with butter butter contains moisture water in fact and that water will actually saw out the crust you may however if you have someone hand brush it with a little rendered bacon fat or a goose fat or duck fat or any of those fats so our oven is still at 450 degrees we're gonna put these back in for 10 minutes just to give it that flash cooking that's gonna give us our crispy skin our potatoes only have a few more minutes in the oven let's throw together a quick topping to put on there fantastic all right so we have four ounces of goat cheese that I'm gonna smash up here we're gonna put in two tablespoons of extra-virgin olive oil two tablespoons of parsley 1 tablespoon of shallots and a half a teaspoon of lemon zest this is my favorite part you know I never would have thought of lemon zest on a baked potato but it makes sense because roasted potatoes you add lemon zest to it why not a baked potato why not baked potatoes lemon zest asked the little Zing's at everything finally we're just gonna season to taste with a little bit of salt and pepper it's very flavorful if it doesn't need a lot sometimes goat cheese is super salty and sometimes it's not very seasoned at all so that's a really good idea mm-hmm and here's the taste part just make sure it's good to go lovely season to ELLs taste so that's it that's it yeah I do you more yeah it's pretty too okay I'm gonna go grab these potatoes out of the other oh look how crisp those skins look very different from when they went in yeah all right so we're gonna tend to these potatoes immediately we are not going to squeeze them to tell if they're ready because we temp them at 205 degrees and people think if they leave them here on the counter these poked holes will let all the steam escape those are all false statements you must attend them immediately cutting them open letting that steam out and getting your beautiful potato when you cut that open it sounds like a potato chip you need it because it was crispy I was just cutting an X across the top come on Oh give it a little squeeze you feel the steam coming out can you imagine if that steam had stayed inside the potato has nowhere to escape just gets super dense without fluffy you can smell them we got crispy we've got flavor yeah we got topping we've got snacks yeah Lizzy they are gorgeous fluffy fluffy and I Noel loves me because she just shoved butter inside that baked potato all right will you do the honors of the goat cheese I sure will hot hot Oh what does that remind mam that texture I mean it's almost whipped it's so light and airy and it's actually soft all the way to the middle that's impressive I want to tear a little bit of the skin here I got a try a piece really really crisp very seasoned - that was that brine who would have thought what are we gonna brine next two plates maybe well thank you so much you totally owned fake potatoes today it was my pleasure well 200 pounds of spuds later we know the way to foolproof baked potatoes hook a few holes in your high starch and russet potatoes and dip them in salt water before baking to season the skin bake them to 205 degrees and oil the skins and return them to the oven until super crisp finally cut the potatoes open as soon as they're out of the oven to let excess steam escape and there you have it from our Test Kitchen to your kitchen proof that this old dog can learn a new trick or two the very best baked potatoes thanks for watching America's Test Kitchen what you think we'll leave a comment and let us know which recipes you're excited to make or you can just say hello you can find links to today's recipes and reviews in the video description and don't forget to subscribe to our Channel see you later\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT para NER (Named Entity Recognition)\r\n",
    "\r\n",
    "Se utiliza el modelo de BERT para predecir o taggear las entidades presentes en una oración. Basado en el ejemplo dado en: https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm, trange\r\n",
    "\r\n",
    "# Se lee el dataset\r\n",
    "# Viene de Kaggle: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus#ner_dataset.csv\r\n",
    "dataset = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\r\n",
    "\r\n",
    "# Un detalle importante de este dataset es que las oraciones están concatenadas verticalmente\r\n",
    "# por lo que solo se le asigna el tag de a que oración pertenece a la primera palabra de la oración. \r\n",
    "# Se debe tomar esto en cuenta y rellenar las tags faltantes usando pandas. \r\n",
    "dataset = dataset.fillna(method=\"ffill\")\r\n",
    "\r\n",
    "# Se visualiza el dataset\r\n",
    "# Tiene 4 columnas:\r\n",
    "# - Oración a la que pertence la palabra\r\n",
    "# - Palabra de oración\r\n",
    "# - POS: Tipo de sintáxis de una palabra en la oración (Part of Sentence)\r\n",
    "# - Tag: Tipo de palabra (semántica)\r\n",
    "dataset.head(18)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O\n",
       "15  Sentence: 1            the   DT      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "17  Sentence: 1             of   IN      O"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Se crea una clase para convertir todas las palabras de una oración a una lista\r\n",
    "# También se concatenan las \"POS\" y \"Tags\" que acompañan a los datos.\r\n",
    "class SentenceGetter(object):\r\n",
    "\r\n",
    "    def __init__(self, data):\r\n",
    "\r\n",
    "        # Número de listas \"enviadas\" o impresas\r\n",
    "        self.n_sent = 0\r\n",
    "\r\n",
    "        # Se crean las variables internas de la función\r\n",
    "        self.data = data\r\n",
    "        self.empty = False\r\n",
    "\r\n",
    "        # Pasos:\r\n",
    "        # 1. Se convierten las columnas de \"Word\", \"POS\" y \"Tags\" en listas\r\n",
    "        # 2. Las listas se convierten en un iterador de tuplas con tres elementos (Word, POS, Tags)\r\n",
    "        # 3. Se convierte el iterador en una lista\r\n",
    "        aggregate_func = lambda sen: list(\r\n",
    "            zip(sen[\"Word\"].values.tolist(), \r\n",
    "                sen[\"POS\"].values.tolist(), \r\n",
    "                sen[\"Tag\"].values.tolist())\r\n",
    "        )\r\n",
    "\r\n",
    "        # Se agrupa la data original de acuerdo a la oración a la que pertenece\r\n",
    "        # Para los valores de cada oración, estos se colocan en listas de tuplas\r\n",
    "        # como se especificó arriba. Los elementos de la lista se pueden indexar\r\n",
    "        # escribiendo: \"grouped_data['Sentence 1']\" por ejemplo.\r\n",
    "        self.grouped_data = self.data.groupby(\"Sentence #\").apply(aggregate_func)\r\n",
    "\r\n",
    "        # Se convierte la serie en una lista\r\n",
    "        self.sentence_values = self.grouped_data.to_list()\r\n",
    "\r\n",
    "        # Se extraen únicamente las palabras\r\n",
    "        self.sentence = [[word[0] for word in sentence_values] for sentence_values in self.sentence_values]\r\n",
    "        self.POS      = [[word[1] for word in sentence_values] for sentence_values in self.sentence_values]\r\n",
    "        self.tags     = [[word[2] for word in sentence_values] for sentence_values in self.sentence_values]\r\n",
    "\r\n",
    "    # Rutina para obtener la siguiente oración en la secuencia\r\n",
    "    def get_next(self):\r\n",
    "\r\n",
    "        try:   \r\n",
    "            # Se retorna el elemento \"n_sent\" de la lista\r\n",
    "            s = self.grouped_data[f\"Sentence: {self.n_sent}\"]\r\n",
    "\r\n",
    "            # Se incrementa el index utilizado\r\n",
    "            self.n_sent += 1\r\n",
    "            return s\r\n",
    "\r\n",
    "        except Exception as e:\r\n",
    "            return None\r\n",
    "\r\n",
    "\r\n",
    "# Se crea una instancia de la clase anterior\r\n",
    "# (Se procesan los datos del dataset)\r\n",
    "sentenceGetter = SentenceGetter(dataset)\r\n",
    "\r\n",
    "# Se extrae la oración 1\r\n",
    "sentenceGetter.sentence[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'British',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se extraen los valores únicos para las diferentes tags existentes en el dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Clases únicas (tags) en el dataset\r\n",
    "# - geo: Entidad geográfica\r\n",
    "# - org: Organización\r\n",
    "# - per: Persona\r\n",
    "# - gpe: Entidad geopolítica\r\n",
    "# - tim: Indicador de tiempo\r\n",
    "# - art: Artefacto\r\n",
    "# - eve: Evento\r\n",
    "# - nat: Fenómeno natural\r\n",
    "# Se extraen los valores de la columna \"Tag\" y luego se eliminan los repetidos\r\n",
    "unique_tags = list(set(dataset[\"Tag\"].values))\r\n",
    "\r\n",
    "# Se adiciona una clase única para palabras de padding\r\n",
    "unique_tags.append(\"PAD\")\r\n",
    "\r\n",
    "# Vector para mapear cada clase a un index diferente\r\n",
    "# (Se le asigna un número a cada clase)\r\n",
    "tag_codes = {tag: idx for idx, tag in enumerate(unique_tags)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se prepara Pytorch y BERT para ser utilizado"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import torch\r\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
    "from transformers import BertTokenizer, BertConfig\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Configuraciones (Hiperparámetros)\r\n",
    "# Max Length: Largo máximo de una secuencia en número de tokens (BERT soporta hasta 512 tokens)\r\n",
    "# Batch Size: Número de muestras pasadas a la vez al algoritmo (32 es una sugerencia del paper de BERT)\r\n",
    "max_len = 75\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "# Se intenta configurar la GPU del sistema. Si no lo consigue\r\n",
    "# entonces utiliza el CPU y se imprime el nombre del procesador utilizado\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(\"Number of Devices:\", torch.cuda.device_count())\r\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))\r\n",
    "\r\n",
    "# BERT ya viene con un tokenizador pre-entrenado y con un vocabulario definido\r\n",
    "# - Se carga el modelo más pequeño: \"bert-base-uncased\"\r\n",
    "# - Se llevan todas las letras a minúsculas: do_lower_case = True\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Devices: 1\n",
      "Device Name: NVIDIA GeForce GTX 960M\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se tokenizan las palabras de cada oración"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Se tokenizan todas las oraciones. Debido a que BERT está basado en el tokenizador\r\n",
    "# de Wordpiece, el mismo va a separar tokens en subpalabras (cada una con su propio\r\n",
    "# token). Por ejemplo, \"gunships\" lo va a separar en \"guns\" y \"##hips\". Esto se debería\r\n",
    "# de solucionar utilizando una estructura de datos especial basada en \"label spans\",\r\n",
    "# sin embargo, aquí se hace de manera explícita.\r\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\r\n",
    "    tokenized_sentence = []\r\n",
    "    labels = []\r\n",
    "\r\n",
    "    # Para cada pareja de oración y labels\r\n",
    "    for word, label in zip(sentence, text_labels):\r\n",
    "\r\n",
    "        # Se tokeniza la palabra y se cuenta el número de subpalabras en las que se rompe\r\n",
    "        tokenized_word = tokenizer.tokenize(word)\r\n",
    "        num_subwords = len(tokenized_word)\r\n",
    "\r\n",
    "        # Se incluye la palabra tokenizada en la lista de palabras tokenizadas\r\n",
    "        tokenized_sentence.extend(tokenized_word)\r\n",
    "\r\n",
    "        # Se agrega la misma label a todas las subpalabras creadas\r\n",
    "        # (Sse multiplica por \"num_subwords\" la label actual)\r\n",
    "        labels.extend([label] * num_subwords)\r\n",
    "\r\n",
    "    return tokenized_sentence, labels\r\n",
    "\r\n",
    "# Se obtiene:\r\n",
    "# - Lista de palabras en la oración (sentences)\r\n",
    "# - Lista de tags para cada palabra (labels)\r\n",
    "sentences = sentenceGetter.sentence\r\n",
    "labels = sentenceGetter.tags\r\n",
    "\r\n",
    "# Se aplica la rutina anterior para todas las oraciones\r\n",
    "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sens, labs) for sens, labs in zip(sentences, labels)]\r\n",
    "\r\n",
    "# Se extraen las palabras tokenizadas y las labels \"multiplicadas\"\r\n",
    "tokenized_texts  = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\r\n",
    "tokenized_labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se hace padding a las secuencias para que encajen con el largo máximo especificado arriba (max length)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# Se cortan y se paddean los tokens \r\n",
    "# 1. Se convierten los tokens a \"IDs\"\r\n",
    "# 2. El largo máximo es el previamente especificado\r\n",
    "# 3. Se truncan tokens muy largos y se paddean los valores muy cortos con 0s\r\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\r\n",
    "                          maxlen=max_len, dtype=\"long\", \r\n",
    "                          value=0.0,\r\n",
    "                          truncating=\"post\", padding=\"post\")\r\n",
    "\r\n",
    "# Se padean las secuencias las tags\r\n",
    "# 1. Se obtienen los IDs de las clases de cada label\r\n",
    "# 2. Se cortan al largo deseado\r\n",
    "# 3. Se truncan valores largos y se paddean valores con \"PAD\".\r\n",
    "tags = pad_sequences([[tag_codes.get(label) for label in sentence_labels] for sentence_labels in labels],\r\n",
    "                     maxlen=max_len, dtype=\"long\", \r\n",
    "                     value=tag_codes[\"PAD\"],\r\n",
    "                     truncating=\"post\",  padding=\"post\")\r\n",
    "\r\n",
    "# BERT soporta algo denominado \"attention masks\", lo cual le permite\r\n",
    "# ignorar padding en una secuencia. Se crean máscaras para ignorar el padding.\r\n",
    "# 1. Se obtienen los IDs para una oración\r\n",
    "# 2. Se chequea si cada ID es diferente de 0.\r\n",
    "# 3. Si es igual a 0, se retorna False y se convierte a 0.0. Los demás valores se convierten en 1.0\r\n",
    "attention_masks = [[float(id != 0.0) for id in input_id] for input_id in input_ids]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se crea el train-test split y se adaptan los valores a elementos utilizables por PyTorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Split del dataset para utilizar el 10% para validación\r\n",
    "train_ids, valid_ids, train_tags, valid_tags = train_test_split(input_ids, tags, random_state=6969, test_size=0.1)\r\n",
    "\r\n",
    "# Se realiza el mismo split para las máscaras de atención\r\n",
    "train_masks, valid_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=6969, test_size=0.1)\r\n",
    "\r\n",
    "# Se convierte el dataset en tensores de PyTorch\r\n",
    "train_ids   = torch.tensor(train_ids)\r\n",
    "train_tags  = torch.tensor(train_tags)\r\n",
    "train_masks = torch.tensor(train_masks)\r\n",
    "valid_ids   = torch.tensor(valid_ids)\r\n",
    "valid_tags  = torch.tensor(valid_tags)\r\n",
    "valid_masks = torch.tensor(valid_masks)\r\n",
    "\r\n",
    "# Se convierten los datos de entrenamiento y de validación \r\n",
    "# a datos utilizables por Pytorch\r\n",
    "# - Se define un dataset\r\n",
    "# - Se crea un \"mezclador\" de datos (RandomSampler)\r\n",
    "# - Se crea un \"cargador\" de datos\r\n",
    "train_dataset    = TensorDataset(train_ids, train_masks, train_tags)\r\n",
    "train_sampler    = RandomSampler(train_dataset)\r\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\r\n",
    "\r\n",
    "valid_dataset    = TensorDataset(valid_ids, valid_masks, valid_tags)\r\n",
    "valid_sampler    = RandomSampler(valid_dataset)\r\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup de BERT para realizar finetuning. Si se carga el modelo pre-entrenado para re-entrenarlo, los comandos van a retornar un mensaje de error. Este puede ser ignorado si el modelo aún no se ha entrenado."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "import transformers\r\n",
    "from transformers import BertForTokenClassification, AdamW\r\n",
    "from transformers import get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "# El paquete de \"transformers\" provee una clase de \"BertForTokenClassification\", la cual\r\n",
    "# consiste de un modelo para \"fine-tuning\" que agrega un clasificador de tokens al modelo\r\n",
    "# base de BERT. El clasificador consiste de una capa lineal que toma como input el último\r\n",
    "# estado oculto de la secuencia. A continuación se carga el modelo pre-entrenado:\r\n",
    "# - Se utiliza el modelo base para palabras en minúscula\r\n",
    "# - Se indica el número de labels a predecir\r\n",
    "# - No se sacan las máscaras de atención ni los estados ocultos\r\n",
    "model = BertForTokenClassification.from_pretrained(\r\n",
    "    \"bert-base-uncased\",\r\n",
    "    num_labels = len(tag_codes),\r\n",
    "    output_attentions = False,\r\n",
    "    output_hidden_states = False\r\n",
    ")\r\n",
    "\r\n",
    "# Se alimentan los parámetros del modelo a la GPU\r\n",
    "model.cuda()\r\n",
    "\r\n",
    "# Se hace el setup del optimizador (ADAM) y se añaden los parámetros que debería de\r\n",
    "# actualizar PyTorch. También se agrega \"weight decay\" para regularizar las matrices\r\n",
    "# principales de pesos. Si no se tienen suficientes recursos, se puede setear\r\n",
    "# full_finetuning a False y así se congelan los pesos de BERT. \r\n",
    "full_finetuning = True\r\n",
    "\r\n",
    "if full_finetuning:\r\n",
    "\r\n",
    "    # Parámetros a optimizar\r\n",
    "    param_optimizer = list(model.named_parameters())\r\n",
    "\r\n",
    "    # Valores a los que no se les aplicará decay\r\n",
    "    no_decay = [\"bias\", \"gamma\", \"beta\"]\r\n",
    "\r\n",
    "    # Se setea el decay para cada grupo de parámetros\r\n",
    "    optimizer_grouped_parameters = [\r\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n",
    "         'weight_decay_rate': 0.01},\r\n",
    "        {'params': [p for n, p in param_optimizer if     any(nd in n for nd in no_decay)],\r\n",
    "         'weight_decay_rate': 0.0}\r\n",
    "    ]\r\n",
    "\r\n",
    "else:\r\n",
    "    # Solo se optimizan los parámetros \r\n",
    "    param_optimizer = list(model.classifier.named_parameters())\r\n",
    "    optimizer_grouped_parameters = [\r\n",
    "        {\"params\": [p for n,p in param_optimizer]}\r\n",
    "    ]\r\n",
    "\r\n",
    "# Se configura el optimizador a utilizar\r\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\r\n",
    "\r\n",
    "# Hiperparámetros de entrenamiento\r\n",
    "epochs = 3\r\n",
    "max_grad_norm = 1.0\r\n",
    "\r\n",
    "# Número total de pasos de entrenamiento:\r\n",
    "# Número de batches * número de epochs\r\n",
    "total_training_steps = len(train_dataloader) * epochs\r\n",
    "\r\n",
    "# Se agrega un scheduler para reducir linealmente el learning rate\r\n",
    "# a lo largo de las epochs.\r\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_training_steps)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se configura BERT para NER (Named Entity Recognition)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# pip install seqeval\r\n",
    "from seqeval.metrics import f1_score, accuracy_score\r\n",
    "\r\n",
    "# Se guarda el costo promedio luego de cada epoch para plotearla\r\n",
    "train_loss_values, validation_loss_values = [], []\r\n",
    "\r\n",
    "# Barra de progreso de TQDM incluida dentro del for\r\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\r\n",
    "\r\n",
    "    # =================================\r\n",
    "    # ENTRENAMIENTO\r\n",
    "    # =================================\r\n",
    "\r\n",
    "    # Se coloca el modelo en \"training mode\"\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    # Se resetea el costo para la epoch actual\r\n",
    "    total_loss = 0\r\n",
    "\r\n",
    "    # Loop de entrenamiento\r\n",
    "    for step, batch in enumerate(train_dataloader):\r\n",
    "\r\n",
    "        # Se agrega el batch actual a la GPU\r\n",
    "        batch = tuple(t.to(device) for t in batch)\r\n",
    "\r\n",
    "        # Se extraen los IDs, input mask y labels del batch\r\n",
    "        batch_ids, batch_mask, batch_labels = batch\r\n",
    "\r\n",
    "        # Se limpian los gradientes calculados\r\n",
    "        model.zero_grad()\r\n",
    "\r\n",
    "        # Forward prop\r\n",
    "        outputs = model(batch_ids, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)\r\n",
    "\r\n",
    "        # Se obtiene el costo de la salida\r\n",
    "        loss = outputs[0]\r\n",
    "\r\n",
    "        # Backward prop\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        # Se toma nota del costo de entrenamiento\r\n",
    "        total_loss += loss.item()\r\n",
    "\r\n",
    "        # Se evitan \"exploding gradients\" al cortar la normal del gradiente\r\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\r\n",
    "\r\n",
    "        # Se actualizan los parámetros usando los gradientes\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        # Se actualiza el learning rate (decay)\r\n",
    "        scheduler.step()\r\n",
    "\r\n",
    "    # Se calcula el costo promedio de entrenamiento\r\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\r\n",
    "    print(f\"Costo Promedio de Entrenamiento: {avg_train_loss}\")\r\n",
    "\r\n",
    "    # Se guarda el valor de costo para plotear la curva de aprendizaje\r\n",
    "    train_loss_values.append(avg_train_loss)\r\n",
    "\r\n",
    "    # =================================\r\n",
    "    # VALIDACIÓN\r\n",
    "    # =================================\r\n",
    "\r\n",
    "    # Se mide el desempeño del modelo en el set de validación\r\n",
    "    \r\n",
    "    # Se setea el modelo en modo de evaluación\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    # Se reinicia el costo de validación\r\n",
    "    val_loss, val_accuracy = 0, 0\r\n",
    "\r\n",
    "    # Arrays para predicciones y labels\r\n",
    "    pred_labels, true_labels = [], []\r\n",
    "\r\n",
    "    # Por cada batch\r\n",
    "    for batch in valid_dataloader:\r\n",
    "\r\n",
    "        # Se agrega el batch actual a la GPU\r\n",
    "        # Se extraen los IDs, input mask y labels del batch\r\n",
    "        batch = tuple(t.to(device) for t in batch)\r\n",
    "        batch_ids, batch_mask, batch_labels = batch\r\n",
    "\r\n",
    "        # Se indica al modelo que no calcule o guarde gradientes para\r\n",
    "        # ahorrar memoria y acelerar la validación.\r\n",
    "        with torch.no_grad():\r\n",
    "\r\n",
    "            # Forward prop.\r\n",
    "            # Esto va a retornar logits en lugar de costo debido a que no se proveyeron labels\r\n",
    "            outputs = model(batch_ids, token_type_ids=None, attention_mask=batch_mask, labels=batch_labels)\r\n",
    "\r\n",
    "        # Se mueven los logits y labels al CPU\r\n",
    "        logits = outputs[1].detach().cpu().numpy()\r\n",
    "        label_ids = batch_labels.to(\"cpu\").numpy()\r\n",
    "\r\n",
    "        # Se calcula la media del costo\r\n",
    "        val_loss += outputs[0].mean().item()\r\n",
    "\r\n",
    "        # Se añaden las labels predichas y las reales a la lista \"global\"\r\n",
    "        pred_labels.extend([list(pred) for pred in np.argmax(logits, axis=2)])\r\n",
    "        true_labels.extend(label_ids)\r\n",
    "\r\n",
    "    # Costo de validación promedio de epoch\r\n",
    "    val_loss = val_loss / len(valid_dataloader)\r\n",
    "    validation_loss_values.append(val_loss)\r\n",
    "    print(f\"Validation Loss: {val_loss}\")\r\n",
    "    \r\n",
    "    # Se obtienen las tags predichas y las reales\r\n",
    "    # Se ignoran las predicciones que corresponden a tags de padding\r\n",
    "    pred_tags  = [unique_tags[p_i] for p,l in zip(pred_labels, true_labels)\r\n",
    "                                  for p_i, l_i in zip(p, l) if unique_tags[l_i] != \"PAD\"]\r\n",
    "\r\n",
    "    valid_tags = [unique_tags[l_i] for l   in zip(pred_labels, true_labels)\r\n",
    "                                  for l_i in l if unique_tags[l_i] != \"PAD\"]\r\n",
    "\r\n",
    "    # Se imprimen los scores\r\n",
    "    print(f\"Validation Accuracy: {accuracy_score(pred_tags, valid_tags)}\")\r\n",
    "    print(f\"Validation F1-Score: {f1_score(pred_tags, valid_tags)}\")\r\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch:   0%|          | 0/3 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.13 GiB already allocated; 0 bytes free; 1.30 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-fc779c2ce5ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Forward prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Se obtiene el costo de la salida\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1720\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1721\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    988\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         )\n\u001b[1;32m--> 990\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    580\u001b[0m                 )\n\u001b[0;32m    581\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     ):\n\u001b[1;32m--> 401\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mkey_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.13 GiB already allocated; 0 bytes free; 1.30 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se visualizan los costos de entrenamiento y de validación"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# Se utiliza el estilo de plot de seaborn\r\n",
    "sns.set(style=\"darkgrid\")\r\n",
    "\r\n",
    "# Se incrementa el tamaño de plot y el tamaño de letra\r\n",
    "sns.set(font_scale=1.5)\r\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
    "\r\n",
    "# Se plotean las curvas de aprendizaje\r\n",
    "plt.plot(train_loss_values, 'b-o', label=\"Training Loss\")\r\n",
    "plt.plot(validation_loss_values, 'b-o', label=\"Validation Loss\")\r\n",
    "plt.title(\"Learning Curves\")\r\n",
    "plt.xlabel(\"Epochs\")\r\n",
    "plt.ylabel(\"Loss\")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se realiza una predicción en una nueva oración"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_sentence = \"\"\"\r\n",
    "Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a \r\n",
    "reporter for the network, about protests in Minnesota and elsewhere. \r\n",
    "\"\"\"\r\n",
    "\r\n",
    "# Se tokeniza el texto y se carga en la GPU\r\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\r\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()\r\n",
    "\r\n",
    "# Se procesa la oración con el modelo (el modelo no debe de actualizar gradientes)\r\n",
    "with torch.no_grad():\r\n",
    "    output = model(input_ids)\r\n",
    "\r\n",
    "# Se obtienen los indices para las labels\r\n",
    "label_indices = np.argmax(output[0].to(\"cpu\").numpy(), axis=2)\r\n",
    "\r\n",
    "# Se vuelven a unir los tokens separados durante el procesamiento\r\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to(\"cpu\").numpy()[0])\r\n",
    "\r\n",
    "# Listas para los nuevos tokens y labels luego de \"re-unir\" los \r\n",
    "# tokens spliteados.\r\n",
    "new_tokens, new_labels = [], []\r\n",
    "\r\n",
    "# Si los tokens fueron separados (Inician con ##), se suman los \r\n",
    "# caracteres distintos de \"##\" al \"token base\". Si no inicia con\r\n",
    "# esos caracteres, entonces simplemente se agregan al registro.\r\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\r\n",
    "    \r\n",
    "    if token.startswith(\"##\"):\r\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\r\n",
    "    else:\r\n",
    "        new_labels.append(unique_tags[label_idx])\r\n",
    "        new_tokens.append(token)\r\n",
    "\r\n",
    "# Se imprime la clasificación de la oración\r\n",
    "for token, label in zip(new_tokens, new_labels):\r\n",
    "    print(f\"{label}\\t{token}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "658dc12c475a3a8caebf03b24f414cffa2901ebd330ffd26b9c22f028a90850c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}